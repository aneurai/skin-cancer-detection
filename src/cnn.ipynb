{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "332ecc8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_2444\\661706313.py:12: DtypeWarning: Columns (51,52) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('../data/train-metadata.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Malignant: 393\n",
      "Total Benign: 400666\n",
      "Training Set: 1793 images (293 Malignant)\n",
      "Val Set: 1050 images (50 Malignant)\n",
      "Test Set: 1050 images (50 Malignant)\n",
      "Starting Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:29<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]\n",
      "Train Loss: 3.4185 | Train Acc: 79.48%\n",
      "Val Loss:   0.6411 | Val Acc:   94.19%\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:19<00:00,  3.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10]\n",
      "Train Loss: 1.2880 | Train Acc: 81.65%\n",
      "Val Loss:   2.2017 | Val Acc:   67.52%\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:19<00:00,  3.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10]\n",
      "Train Loss: 1.1523 | Train Acc: 82.93%\n",
      "Val Loss:   1.0161 | Val Acc:   76.10%\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:19<00:00,  2.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10]\n",
      "Train Loss: 0.4713 | Train Acc: 86.11%\n",
      "Val Loss:   0.7090 | Val Acc:   79.62%\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:18<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10]\n",
      "Train Loss: 0.3056 | Train Acc: 89.85%\n",
      "Val Loss:   0.6793 | Val Acc:   78.57%\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:18<00:00,  3.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10]\n",
      "Train Loss: 0.2811 | Train Acc: 90.13%\n",
      "Val Loss:   0.9015 | Val Acc:   72.10%\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:18<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10]\n",
      "Train Loss: 0.3165 | Train Acc: 89.18%\n",
      "Val Loss:   1.1130 | Val Acc:   65.52%\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:18<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10]\n",
      "Train Loss: 0.3751 | Train Acc: 86.84%\n",
      "Val Loss:   0.3486 | Val Acc:   88.00%\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:18<00:00,  3.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10]\n",
      "Train Loss: 0.2399 | Train Acc: 91.58%\n",
      "Val Loss:   0.2852 | Val Acc:   90.10%\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:19<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10]\n",
      "Train Loss: 0.1893 | Train Acc: 92.92%\n",
      "Val Loss:   0.2920 | Val Acc:   89.81%\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from custom_datasets import SkinLesionDataset\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.read_csv('../data/train-metadata.csv')\n",
    "\n",
    "df_malignant = df[df['target'] == 1]\n",
    "df_benign = df[df['target'] == 0]\n",
    "\n",
    "print(f\"Total Malignant: {len(df_malignant)}\")\n",
    "print(f\"Total Benign: {len(df_benign)}\")\n",
    "\n",
    "test_mal, train_val_mal = train_test_split(df_malignant, test_size=None, train_size=50)\n",
    "test_ben, train_val_ben = train_test_split(df_benign, test_size=None, train_size=1000)\n",
    "\n",
    "val_mal, train_mal = train_test_split(train_val_mal, test_size=None, train_size=50)\n",
    "val_ben, train_ben = train_test_split(train_val_ben, test_size=None, train_size=1000)\n",
    "\n",
    "# 5. Create Training Set (The Balancing Act)\n",
    "# We now have ~300 Malignant images left.\n",
    "# We have ~398,000 Benign images left.\n",
    "# WE CANNOT USE ALL BENIGN IMAGES. It will drown out the signal.\n",
    "\n",
    "# Downsample Benign to a 1:5 ratio (300 Malignant : 1500 Benign)\n",
    "# This gives the model a chance to actually see the cancer.\n",
    "train_ben_downsampled = train_ben.sample(n=1500)\n",
    "\n",
    "# Concatenate back together\n",
    "train_df = pd.concat([train_mal, train_ben_downsampled])\n",
    "val_df = pd.concat([val_mal, val_ben])\n",
    "test_df = pd.concat([test_mal, test_ben])\n",
    "\n",
    "# Shuffle them\n",
    "train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "val_df = val_df.sample(frac=1).reset_index(drop=True)\n",
    "test_df = test_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print(f\"Training Set: {len(train_df)} images ({train_df['target'].sum()} Malignant)\")\n",
    "print(f\"Val Set: {len(val_df)} images ({val_df['target'].sum()} Malignant)\")\n",
    "print(f\"Test Set: {len(test_df)} images ({test_df['target'].sum()} Malignant)\")\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((128, 128)),\n",
    "    # other augmentations for train dataset\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], # ImageNet mean and std\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_ds = SkinLesionDataset(dataframe=train_df,\n",
    "                             root_dir=Path('../data/train-image/image'),\n",
    "                             transforms=train_transforms)\n",
    "val_ds = SkinLesionDataset(dataframe=val_df,\n",
    "                           root_dir=Path('../data/train-image/image'),\n",
    "                           transforms=val_transforms)\n",
    "test_ds = SkinLesionDataset(dataframe=test_df,\n",
    "                            root_dir=Path('../data/train-image/image'),\n",
    "                            transforms=test_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32)\n",
    "val_loader = DataLoader(val_ds, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=32, shuffle=False)\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, \n",
    "                               out_channels=32, \n",
    "                               kernel_size=3, \n",
    "                               padding=1) # 32, 128, 128\n",
    "        self.batchNorm1 = nn.BatchNorm2d(num_features=32)\n",
    "        self.relu1 = nn.ReLU(inplace=True) # inplace saves gpu memory (vram), modifies input tensor directly in memory rather than creating a new tensor for the output\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2) # 32, 64, 64\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(32 * 64 * 64, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.batchNorm1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.flatten(x)\n",
    "        return self.fc(x)\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SimpleCNN().to(device)\n",
    "# maybe add pos_weight to tell model to pay more attention to malignant cases\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in tqdm(loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(images) \n",
    "        \n",
    "        loss = criterion(outputs, labels.view(-1, 1).float())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        predicted = torch.sigmoid(outputs) > 0.5\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted.view(-1) == labels).sum().item()\n",
    "        \n",
    "    avg_loss = running_loss / len(loader)\n",
    "    acc = 100 * correct / total\n",
    "    return avg_loss, acc\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels.view(-1, 1).float())\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            predicted = torch.sigmoid(outputs) > 0.5\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted.view(-1) == labels).sum().item()\n",
    "            \n",
    "    avg_loss = running_loss / len(loader)\n",
    "    acc = 100 * correct / total\n",
    "    return avg_loss, acc\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "print(\"Starting Training...\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}]\")\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.2f}%\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783872d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
